---
title: "New Downloading Interface"
output: pdf_document
---

The data is stored in individually compressed files at https://nhorton.people.amherst.edu/valleybikes/ .

```{r}
library(readr)
```

These is how we would read with readr.

```{r}
base_dir <- "https://nhorton.people.amherst.edu/valleybikes/"

Day <- read_csv(paste0(base_dir, "VB_Routes_Data_2018_06_30.csv.gz"),
         skip = 2,
         col_types = cols(
             `Route ID` = col_character(),
             Bike = col_number(),
             Date = col_datetime(format = ""),
             Latitude = col_double(),
             Longitude = col_double(),
             `User ID` = col_character())
         ) %>% janitor::clean_names()
```

However this isn't flexible because it loads the file into memory, while we want to download it.

```{r}
# download a single file. this function should contain check if the file was already downloaded.
fetch_file <- function(file_path, output_dir, replace = FALSE) {
    file_name <- basename(file_path)
    output_path <- file.path(output_dir, file_name)
    if(!file.exists(output_path) || replace) {
        download.file(file_path, destfile = output_path)
    }
}
```
